{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c17e3d74-174d-4d5c-aa0a-326efb80e134",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import pandas as pd\n",
    "import functools\n",
    "import types\n",
    "import os\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "from pandas import *\n",
    "from functools import *\n",
    "from pyspark import *\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import *\n",
    "from datetime import datetime, timedelta\n",
    "from delta.tables import *\n",
    "\n",
    "# Iniciar uma sessão Spark\n",
    "spark = SparkSession.builder.appName(\"tabelas\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66fc8c21-b5a7-4c20-908f-96019793a056",
     "showTitle": true,
     "title": "upsert dim Pedidos"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existem:9635 registros para atualizar\nTabela dim_pedidos atualizada com sucesso\n"
     ]
    }
   ],
   "source": [
    "# criação de variável para filtrar tudo que mudou nas ultimas 48 horas\n",
    "data_limite = datetime.now() - timedelta(hours=72)\n",
    "\n",
    "# acessando a base staging e filtrando os registros conforme tempo de análise\n",
    "df_pedidos = spark.read.table(\"hive_metastore.asap_mvp.dados_pedidos\")\n",
    "\n",
    "# filtrando as ultimas 48 horas\n",
    "df_pedidos = df_pedidos.filter(col(\"ultima_atualizacao\")>=data_limite)\n",
    "\n",
    "# criando tabela temprorária de atualização\n",
    "if df_pedidos.count() == 0:\n",
    "   print(\"Nenhum Update a ser realizado em dim_pedidos\")\n",
    "else:\n",
    "    # Fazer o filtro no DataFrame df_pedidos\n",
    "    filtered_df = df_pedidos\n",
    "\n",
    "    # criando a tabela temporaria para rodar o upsert\n",
    "    db_name = \"asap_update\"\n",
    "    table_name = \"dados_pedidos_update\"\n",
    "\n",
    "    filtered_df.write \\\n",
    "        .format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .option(\"overwriteSchema\", \"true\") \\\n",
    "        .saveAsTable(f\"{db_name}.{table_name}\")\n",
    "\n",
    "    # iniciando o processo de atualização dos dados\n",
    "    deltaTableOficial = DeltaTable.forPath(spark, 'dbfs:/user/hive/warehouse/asap_mvp.db/dim_pedidos')\n",
    "    deltaTableUpdates = DeltaTable.forPath(spark, 'dbfs:/user/hive/warehouse/asap_update.db/dados_pedidos_update')\n",
    "\n",
    "    dfUpdates = deltaTableUpdates.toDF()\n",
    "    print(\"Existem:\"+str(dfUpdates.count())+\" registros para atualizar\")\n",
    "    deltaTableOficial.alias('oficial') \\\n",
    "    .merge(\n",
    "    dfUpdates.alias('updates'),\n",
    "        'oficial.id_pedido = updates.id_pedido'\n",
    "    ) \\\n",
    "    .whenMatchedUpdate(set =\n",
    "    {\n",
    "        \"oficial.id_pedido\": \"updates.id_pedido\",\n",
    "        \"oficial.codigoCliente\": \"updates.codigoCliente\",\n",
    "        \"oficial.data_criacao\": \"updates.data_criacao\",\n",
    "        \"oficial.ultima_atualizacao\": \"updates.ultima_atualizacao\",\n",
    "        \"oficial.data_planejada\": \"updates.data_planejada\",\n",
    "        \"oficial.data_previsao_entrega\": \"updates.data_previsao_entrega\",\n",
    "        \"oficial.prazo_maximo_entrega\": \"updates.prazo_maximo_entrega\",\n",
    "        \"oficial.tipo_entrega\": \"updates.tipo_entrega\",\n",
    "        \"oficial.tipo_transferencia\": \"updates.tipo_transferencia\",\n",
    "        \"oficial.tipo_operacao\": \"updates.tipo_operacao\",\n",
    "        \"oficial.status_pedido\": \"updates.status_pedido\",\n",
    "        \"oficial.valor_nota\": \"updates.valor_nota\",\n",
    "        \"oficial.estado_origem\": \"updates.estado_origem\",\n",
    "        \"oficial.destino_estado\": \"updates.destino_estado\",\n",
    "        \"oficial.destino_cidade\": \"updates.destino_cidade\",\n",
    "        \"oficial.destino_cep\": \"updates.destino_cep\"\n",
    "    }\n",
    "    ) \\\n",
    "    .whenNotMatchedInsert(values =\n",
    "    {\n",
    "        \"oficial.id_pedido\": \"updates.id_pedido\",\n",
    "        \"oficial.codigoCliente\": \"updates.codigoCliente\",\n",
    "        \"oficial.data_criacao\": \"updates.data_criacao\",\n",
    "        \"oficial.ultima_atualizacao\": \"updates.ultima_atualizacao\",\n",
    "        \"oficial.data_planejada\": \"updates.data_planejada\",\n",
    "        \"oficial.data_previsao_entrega\": \"updates.data_previsao_entrega\",\n",
    "        \"oficial.prazo_maximo_entrega\": \"updates.prazo_maximo_entrega\",\n",
    "        \"oficial.tipo_entrega\": \"updates.tipo_entrega\",\n",
    "        \"oficial.tipo_transferencia\": \"updates.tipo_transferencia\",\n",
    "        \"oficial.tipo_operacao\": \"updates.tipo_operacao\",\n",
    "        \"oficial.status_pedido\": \"updates.status_pedido\",\n",
    "        \"oficial.valor_nota\": \"updates.valor_nota\",\n",
    "        \"oficial.estado_origem\": \"updates.estado_origem\",\n",
    "        \"oficial.destino_estado\": \"updates.destino_estado\",\n",
    "        \"oficial.destino_cidade\": \"updates.destino_cidade\",\n",
    "        \"oficial.destino_cep\": \"updates.destino_cep\"\n",
    "    }\n",
    "    ) \\\n",
    "    .execute()\n",
    "\n",
    "    print(\"Tabela dim_pedidos atualizada com sucesso\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "86ee2280-aff8-4d1a-a561-28057660f936",
     "showTitle": true,
     "title": "upsert dim pedidos filhos"
    }
   },
   "outputs": [],
   "source": [
    "# criação de variável para filtrar tudo que mudou nas ultimas 48 horas\n",
    "data_limite = datetime.now() - timedelta(hours=48)\n",
    "\n",
    "# acessando a base staging e filtrando os registros conforme tempo de análise\n",
    "df_pedidos_filhos = spark.read.table(\"hive_metastore.asap_mvp.dados_pedidos_filhos\")\n",
    "\n",
    "# filtrando as ultimas 48 horas\n",
    "df_pedidos_filhos = df_pedidos_filhos.filter(col(\"ultima_atualizacao\")>=data_limite)\n",
    "\n",
    "if df_pedidos_filhos.count() == 0:\n",
    "   print(\"Nenhum Update a ser realizado\")\n",
    "else:\n",
    "    # iniciando o processo de atualização dos dados\n",
    "    tabela_delta = 'dbfs:/user/hive/warehouse/asap_mvp.db/dim_pedidos_filhos'\n",
    "    df_tabela = spark.read.format('delta').load(tabela_delta)\n",
    "\n",
    "    df_diff = df_pedidos_filhos.join(df_tabela, (df_pedidos_filhos.id_pedido_filho == df_tabela.id_pedido_filho)\\\n",
    "            & (df_pedidos_filhos.codigoRoteiros == df_tabela.codigoRoteiros), \"left_anti\")\n",
    "    \n",
    "    if df_diff.count() ==0:\n",
    "        print(\"Nada a ser inserido\")\n",
    "    else:\n",
    "        # Realizar o append na tabela Delta\n",
    "        df_diff.write.format(\"delta\").mode(\"append\").save(tabela_delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9b23ffcd-486d-4761-8946-666753ac3186",
     "showTitle": true,
     "title": "upsert  dim roteiros"
    }
   },
   "outputs": [],
   "source": [
    "# criação de variável para filtrar tudo que mudou nas ultimas 48 horas\n",
    "data_limite = datetime.now() - timedelta(hours=48)\n",
    "\n",
    "# acessando a base staging e filtrando os registros conforme tempo de análise\n",
    "df_roteiros = spark.read.table(\"hive_metastore.asap_mvp.dados_roteiros\")\n",
    "\n",
    "# filtrando as ultimas 48 horas\n",
    "df_roteiros = df_roteiros.filter(col(\"lastUpdated\")>=data_limite)\n",
    "\n",
    "# criando tabela temprorária de atualização\n",
    "if df_roteiros.count() == 0:\n",
    "   print(\"Nenhum Update a ser realizado em dim_pedidos\")\n",
    "else:\n",
    "    # Fazer o filtro no DataFrame df_pedidos\n",
    "    filtered_df = df_roteiros\n",
    "\n",
    "    # criando a tabela temporaria para rodar o upsert\n",
    "    db_name = \"asap_update\"\n",
    "    table_name = \"dados_roteiros_update\"\n",
    "\n",
    "    filtered_df.write \\\n",
    "        .format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .option(\"overwriteSchema\", \"true\") \\\n",
    "        .saveAsTable(f\"{db_name}.{table_name}\")\n",
    "\n",
    "    # iniciando o processo de atualização dos dados\n",
    "    deltaTableOficial = DeltaTable.forPath(spark, 'dbfs:/user/hive/warehouse/asap_mvp.db/dim_roteiros')\n",
    "    deltaTableUpdates = DeltaTable.forPath(spark, 'dbfs:/user/hive/warehouse/asap_update.db/dados_roteiros_update')\n",
    "\n",
    "    dfUpdates = deltaTableUpdates.toDF()\n",
    "    print(\"Existem:\"+str(dfUpdates.count())+\" registros para atualizar\")\n",
    "    deltaTableOficial.alias('oficial') \\\n",
    "    .merge(\n",
    "    dfUpdates.alias('updates'),\n",
    "        'oficial.codigoRoteiro = updates.codigoRoteiro'\n",
    "    ) \\\n",
    "    .whenMatchedUpdate(set =\n",
    "    {\n",
    "        \"oficial.codigoRoteiro\": \"updates.codigoRoteiro\",\n",
    "        \"oficial.date\": \"updates.date\",\n",
    "        \"oficial.dateCreated\": \"updates.dateCreated\",\n",
    "        \"oficial.lastUpdated\": \"updates.lastUpdated\",\n",
    "        \"oficial.placaVeiculo\": \"updates.placaVeiculo\",\n",
    "        \"oficial.qtdPedidos\": \"updates.qtdPedidos\",\n",
    "        \"oficial.qtdVolumes\": \"updates.qtdVolumes\",\n",
    "        \"oficial.distanciaKm\": \"updates.distanciaKm\",\n",
    "        \"oficial.regiaoEntregaGuid\": \"updates.regiaoEntregaGuid\"\n",
    "    }\n",
    "    ) \\\n",
    "    .whenNotMatchedInsert(values =\n",
    "    {\n",
    "        \"oficial.codigoRoteiro\": \"updates.codigoRoteiro\",\n",
    "        \"oficial.date\": \"updates.date\",\n",
    "        \"oficial.dateCreated\": \"updates.dateCreated\",\n",
    "        \"oficial.lastUpdated\": \"updates.lastUpdated\",\n",
    "        \"oficial.placaVeiculo\": \"updates.placaVeiculo\",\n",
    "        \"oficial.qtdPedidos\": \"updates.qtdPedidos\",\n",
    "        \"oficial.qtdVolumes\": \"updates.qtdVolumes\",\n",
    "        \"oficial.distanciaKm\": \"updates.distanciaKm\",\n",
    "        \"oficial.regiaoEntregaGuid\": \"updates.regiaoEntregaGuid\"\n",
    "    }\n",
    "    ) \\\n",
    "    .execute()\n",
    "\n",
    "    print(\"Tabela dim_roteiros atualizada com sucesso\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "729283ad-b5f0-46b3-95c0-9be1461368ec",
     "showTitle": true,
     "title": "upsert dim roteiros filhos"
    }
   },
   "outputs": [],
   "source": [
    "# criação de variável para filtrar tudo que mudou nas ultimas 48 horas\n",
    "data_limite = datetime.now() - timedelta(hours=48)\n",
    "\n",
    "# acessando a base staging e filtrando os registros conforme tempo de análise\n",
    "df_roteiros_filhos = spark.read.table(\"hive_metastore.asap_mvp.dados_roteiros_filhos\")\n",
    "\n",
    "# filtrando as ultimas 48 horas\n",
    "df_roteiros_filhos = df_roteiros_filhos.filter(col(\"lastUpdated\")>=data_limite)\n",
    "\n",
    "if df_roteiros_filhos.count() == 0:\n",
    "   print(\"Nenhum Update a ser realizado\")\n",
    "else:\n",
    "    # iniciando o processo de atualização dos dados\n",
    "    tabela_delta = 'dbfs:/user/hive/warehouse/asap_mvp.db/dim_roteiros_filhos'\n",
    "    df_tabela = spark.read.format('delta').load(tabela_delta)\n",
    "\n",
    "    df_diff = df_roteiros_filhos.join(df_tabela, (df_roteiros_filhos.codigoRoteiro == df_tabela.codigoRoteiro)\\\n",
    "            & (df_roteiros_filhos.pedidGuidList == df_tabela.pedidGuidList), \"left_anti\")\n",
    "    \n",
    "    if df_diff.count() ==0:\n",
    "        print(\"Nada a ser inserido na tabela de roteiros filho\")\n",
    "    else:\n",
    "        # Realizar o append na tabela Delta\n",
    "        df_diff.write.format(\"delta\").mode(\"append\").save(tabela_delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0d1077fd-b5f4-4bbd-90dd-4bcefc0d17f3",
     "showTitle": true,
     "title": "upsert dim filial"
    }
   },
   "outputs": [],
   "source": [
    "# acessando a base staging e filtrando os registros conforme tempo de análise\n",
    "df_filial = spark.read.table(\"hive_metastore.asap_mvp.dados_filial\")\n",
    "\n",
    "# criando tabela temprorária de atualização\n",
    "if df_filial.count() == 0:\n",
    "   print(\"Nenhum Update a ser realizado em dim_pedidos\")\n",
    "else:\n",
    "    # Fazer o filtro no DataFrame df_pedidos\n",
    "    filtered_df = df_filial\n",
    "\n",
    "    # criando a tabela temporaria para rodar o upsert\n",
    "    db_name = \"asap_update\"\n",
    "    table_name = \"dados_filial_update\"\n",
    "\n",
    "    filtered_df.write \\\n",
    "        .format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .option(\"overwriteSchema\", \"true\") \\\n",
    "        .saveAsTable(f\"{db_name}.{table_name}\")\n",
    "\n",
    "    # iniciando o processo de atualização dos dados\n",
    "    deltaTableOficial = DeltaTable.forPath(spark, 'dbfs:/user/hive/warehouse/asap_mvp.db/dim_filial')\n",
    "    deltaTableUpdates = DeltaTable.forPath(spark, 'dbfs:/user/hive/warehouse/asap_update.db/dados_filial_update')\n",
    "\n",
    "    dfUpdates = deltaTableUpdates.toDF()\n",
    "    print(\"Existem:\"+str(dfUpdates.count())+\" registros para atualizar\")\n",
    "    deltaTableOficial.alias('oficial') \\\n",
    "    .merge(\n",
    "    dfUpdates.alias('updates'),\n",
    "        'oficial.id_filial = updates.id_filial'\n",
    "    ) \\\n",
    "    .whenMatchedUpdate(set =\n",
    "    {\n",
    "        \"oficial.id_filial\": \"updates.id_filial\",\n",
    "        \"oficial.nome\": \"updates.nome\",\n",
    "        \"oficial.codigoFilial\": \"updates.codigoFilial\",\n",
    "        \"oficial.nomeFantasia\": \"updates.nomeFantasia\"\n",
    "    }\n",
    "    ) \\\n",
    "    .whenNotMatchedInsert(values =\n",
    "    {\n",
    "        \"oficial.id_filial\": \"updates.id_filial\",\n",
    "        \"oficial.nome\": \"updates.nome\",\n",
    "        \"oficial.codigoFilial\": \"updates.codigoFilial\",\n",
    "        \"oficial.nomeFantasia\": \"updates.nomeFantasia\"\n",
    "    }\n",
    "    ) \\\n",
    "    .execute()\n",
    "\n",
    "    print(\"Tabela dim_filial atualizada com sucesso\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ab6e40e7-3cfd-4dac-8eba-5aa5708b8883",
     "showTitle": true,
     "title": "upsert dim regiao"
    }
   },
   "outputs": [],
   "source": [
    "# criação de variável para filtrar tudo que mudou nas ultimas 48 horas\n",
    "data_limite = datetime.now() - timedelta(hours=48)\n",
    "\n",
    "# acessando a base staging e filtrando os registros conforme tempo de análise\n",
    "df_regiao = spark.read.table(\"hive_metastore.asap_mvp.dados_regiao\")\n",
    "\n",
    "# filtrando as ultimas 48 horas\n",
    "df_regiao = df_regiao.filter(col(\"lastUpdated\")>=data_limite)\n",
    "\n",
    "# criando tabela temprorária de atualização\n",
    "if df_regiao.count() == 0:\n",
    "   print(\"Nenhum Update a ser realizado em dim_pedidos\")\n",
    "else:\n",
    "    # Fazer o filtro no DataFrame df_pedidos\n",
    "    filtered_df = df_regiao\n",
    "\n",
    "    # criando a tabela temporaria para rodar o upsert\n",
    "    db_name = \"asap_update\"\n",
    "    table_name = \"dados_regiao_update\"\n",
    "\n",
    "    filtered_df.write \\\n",
    "        .format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .option(\"overwriteSchema\", \"true\") \\\n",
    "        .saveAsTable(f\"{db_name}.{table_name}\")\n",
    "\n",
    "    # iniciando o processo de atualização dos dados\n",
    "    deltaTableOficial = DeltaTable.forPath(spark, 'dbfs:/user/hive/warehouse/asap_mvp.db/dim_regiao')\n",
    "    deltaTableUpdates = DeltaTable.forPath(spark, 'dbfs:/user/hive/warehouse/asap_update.db/dados_regiao_update')\n",
    "\n",
    "    dfUpdates = deltaTableUpdates.toDF()\n",
    "    print(\"Existem:\"+str(dfUpdates.count())+\" registros para atualizar\")\n",
    "    deltaTableOficial.alias('oficial') \\\n",
    "    .merge(\n",
    "    dfUpdates.alias('updates'),\n",
    "        'oficial.id_regiao = updates.id_regiao'\n",
    "    ) \\\n",
    "    .whenMatchedUpdate(set =\n",
    "    {\n",
    "        \"oficial.id_regiao\": \"updates.id_regiao\",\n",
    "        \"oficial.codigoRegiao\": \"updates.codigoRegiao\",\n",
    "        \"oficial.filialId\": \"updates.filialId\",\n",
    "        \"oficial.nome\": \"updates.nome\",\n",
    "        \"oficial.uf\": \"updates.uf\"\n",
    "    }\n",
    "    ) \\\n",
    "    .whenNotMatchedInsert(values =\n",
    "    {\n",
    "        \"oficial.id_regiao\": \"updates.id_regiao\",\n",
    "        \"oficial.codigoRegiao\": \"updates.codigoRegiao\",\n",
    "        \"oficial.filialId\": \"updates.filialId\",\n",
    "        \"oficial.nome\": \"updates.nome\",\n",
    "        \"oficial.uf\": \"updates.uf\"\n",
    "    }\n",
    "    ) \\\n",
    "    .execute()\n",
    "\n",
    "    print(\"Tabela dim_regiao atualizada com sucesso\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 224810749996341,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "upsert dados",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
